{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "homeless-heart",
   "metadata": {},
   "source": [
    "# Learning goals\n",
    "After today's lesson you should be able to:\n",
    "- Understand the \n",
    "\n",
    "This week's lesson is a simplified version of:  \n",
    "- The [Chapter 6 in Geographic Data Science textbook](https://geographicdata.science/book/notebooks/06_spatial_autocorrelation.html)\n",
    "- The [Chapter 7 in Geographic Data Science textbook](https://geographicdata.science/book/notebooks/07_local_autocorrelation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd8e1d",
   "metadata": {},
   "source": [
    "# Global Spatial Autocorrelation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e4e8466",
   "metadata": {},
   "source": [
    "The notion of spatial autocorrelation relates to the existence of a \"functional relationship between what happens at one point in space and what happens elsewhere\" {cite}`Anselin_1988`. **Spatial autocorrelation thus has to do with the degree to which the similarity in values between observations in a dataset is related to the similarity in locations of such observations.** \n",
    "\n",
    "## Understanding Spatial Autocorrelation\n",
    "\n",
    "In order to better understand the notion of spatial autocorrelation, it is useful to begin by considering what the world looks like in its absence. A key idea in this context is that of spatial randomness: **a situation in which the location of an observation gives no information whatsoever about its value**. In other words, a variable is spatially random if its distribution follows no discernible spatial pattern. Spatial autocorrelation can thus be defined as the \"absence of spatial randomness\". \n",
    "\n",
    "This definition is still too vague, though. So, to get more specific, spatial autocorrelation is typically categorized along two main dimensions: **sign** and **scale**. Similar to the traditional, non-spatial case, spatial autocorrelation can adopt two main forms: **positive** and **negative**. The former relates to a situation where similarity and geographical closeness go hand-in-hand. In other words, similar values are located near each other, while different values tend to be scattered and further away. It is important that the sign of these values is not relevant for the presence of spatial autocorrelation: it may be high values close to high values, *or* low values close to low values. The important bit in this context is the relationship between closeness and statistical similarity is positive. This is a fairly common case in many social contexts and, in fact, several human phenomena display  clearly positive spatial autocorrelation. **For example, think of the distribution of income, or poverty, over space: it is common to find similar values located nearby (wealthy areas close to other wealthy areas, poor population concentrated in space too).** In contrast, **negative** spatial autocorrelation reflects a situation where similar values tend to be located away from each other. In this case, statistical similarity is associated with distance. This is somewhat less common in the social sciences, but it still exists. An example can be found in phenomena that follow processes of spatial competition or situations where the location of a set of facilities aims at the highest spatial coverage. **The distribution of supermarkets of different brands, or of hospitals usually follows a pattern of negative spatial dependence.**\n",
    "\n",
    "It can also help to understand spatial autocorrelation using the scale at which it is considered. We generally talk of global or local processes. **Global** spatial autocorrelation, on which this chapter is focused on, considers the overall trend that the location of values follows. In doing this, the study of global spatial autocorrelation makes possible statements about the degree of *clustering* in the dataset. Do values generally follow a particular pattern in their geographical distribution? Are similar values closer to other similar values than we would expect from pure chance? These are some of the questions that relate to global spatial autocorrelation. **Local** autocorrelation focuses on deviations from the global trend at much more focused levels than the entire map, and it is the subject of the next chapter.\n",
    "\n",
    "We will explore these concepts with an applied example, interrogating the data about the presence, nature, and strength of global spatial autocorrelation. To do this, we will use a set of tools collectively known as Exploratory Spatial Data Analysis (ESDA). Analogous to its non-spatial counterpart (EDA; {cite}`Tukey1977exploratory`), ESDA has been specifically designed for this purpose, and puts space and the relative location of the observations in a dataset at the forefront of the analysis. The range of ESDA methods is wide and spans from simpler approaches like choropleth maps (previous chapter), to more advanced and robust methodologies that include statistical inference and an explicit recognition of the geographical arrangement of the data. The purpose of this chapter is to dip our toes into the latter group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91841ab9",
   "metadata": {},
   "source": [
    "## An empirical illustration: the EU Referendum\n",
    "\n",
    "To illustrate the notion of spatial autocorrelation and its different variants, let us turn to an example with real world data. Before the data, let us import all the relevant libraries that we will use throughout the chapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd08f8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numba==0.57.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5b9367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphics\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "import seaborn as sns\n",
    "from pysal.viz import splot\n",
    "from splot.esda import plot_moran\n",
    "# import contextily as ctx\n",
    "\n",
    "sns.set_context(context='paper')\n",
    "\n",
    "\n",
    "# Analysis\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pysal.explore import esda\n",
    "from pysal.lib import weights\n",
    "from numpy.random import seed\n",
    "\n",
    "import rioxarray  # Surface data manipulation\n",
    "import xarray  # Surface data manipulation\n",
    "\n",
    "\n",
    "\n",
    "## warnings is a module that allows you to filter warnings\n",
    "import warnings\n",
    "## we are going to ignore all warnings (so they won't print)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde649a0",
   "metadata": {},
   "source": [
    "In 2016, the United Kingdom ran a referendum to decide whether to remain in the European Union or to leave the club, the so called \"Brexit\" vote. We will use the official data from the Electoral Commission at the local authority level on percentage of votes for the Remain and Leave campaigns. There are two distinct datasets we will combine:\n",
    "\n",
    "* Electoral Commission data on vote percentages at the local authority level. [[CSV]](http://www.electoralcommission.org.uk/__data/assets/file/0014/212135/EU-referendum-result-data.csv)\n",
    "* ONS Local Authority Districts (December 2016) Generalized Clipped Boundaries in the UK WGS84. [[SHP]](https://data.gov.uk/dataset/65f48bab-e65f-491c-90f5-729eef098196/local-authority-districts-december-2016-generalised-clipped-boundaries-in-the-uk-wgs84)\n",
    "\n",
    "The vote results are stored in a `csv` file which we read into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92455cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "brexit_data_path = \"https://www.dropbox.com/s/ogo2voxzkrz3dvz/brexit_vote.csv?dl=1\"\n",
    "ref = pd.read_csv(brexit_data_path, index_col=\"Area_Code\")\n",
    "\n",
    "ref.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0998821",
   "metadata": {},
   "source": [
    "While the shapes of the geographical units (local authority districts, in this case) are stored in a compressed GeoJSON file. We can read it directly from the `.zip` file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4ca615",
   "metadata": {},
   "outputs": [],
   "source": [
    "lads = gpd.read_file(\n",
    "    \"https://www.dropbox.com/s/xek0dim8636clhv/local_authority_districts.geojson?dl=1\"\n",
    ").set_index(\"lad16cd\")\n",
    "lads.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45ecd28",
   "metadata": {},
   "source": [
    "Although there are several variables that could be considered, we will focus on `Pct_Leave`, which measures the proportion of votes for the Leave alternative. For convenience, let us merge the vote results with the spatial data and project the output into the Spherical Mercator coordinate reference system (CRS), the preferred choice of web maps, which will allow us to combine them with contextual tiles later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403898b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = (\n",
    "    gpd.GeoDataFrame(\n",
    "        lads.join(ref[[\"Pct_Leave\"]]), crs=lads.crs\n",
    "    )\n",
    "    .to_crs(epsg=3857)[\n",
    "        [\"objectid\", \"lad16nm\", \"Pct_Leave\", \"geometry\"]\n",
    "    ]\n",
    "    .dropna()\n",
    ")\n",
    "db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ebc82f",
   "metadata": {},
   "source": [
    "And with these elements, we can generate a choropleth map to get a quick sense of the spatial distribution of the data we will be analyzing. Note how we use some visual tweaks (e.g. transparency through the `alpha` attribute) to make the final plot easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f75f320",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "db.plot(\n",
    "    column=\"Pct_Leave\",\n",
    "    cmap=\"viridis\",\n",
    "    scheme=\"quantiles\",\n",
    "    k=5,\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=0.0,\n",
    "    alpha=0.75,\n",
    "    legend=True,\n",
    "    legend_kwds={\"loc\": 2},\n",
    "    ax=ax,\n",
    ")\n",
    "# ctx.add_basemap(\n",
    "#     ax,\n",
    "#     crs=db.crs,\n",
    "#     source=ctx.providers.Stamen.TerrainBackground,\n",
    "# )\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd68be57",
   "metadata": {},
   "source": [
    "The final piece we need before we can delve into autocorrelation is the spatial weights matrix. We will use eight nearest neighbors for the sake of the example, but the discussion in the earlier chapter on weights applies in this context, and other criteria would be valid too. We also row-standardize them. \n",
    "\n",
    "**Row standardization is a way of normalizing the weights so that they sum to 1 for each observation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42908352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate W from the GeoDataFrame\n",
    "w = weights.KNN.from_dataframe(db, k=8)\n",
    "# Row-standardization\n",
    "w.transform = \"R\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c1837ae",
   "metadata": {},
   "source": [
    "## Global spatial autocorrelation\n",
    "\n",
    "The map above is a good way to begin exploring the main spatial patterns in the data. At first sight, it appears to display a fair amount of positive spatial autocorrelation: local authorities with high percentages of votes to leave the EU tend to be next to each other (see, for instance, the eastern region), as are those where a much smaller proportion of their population voted to leave (with Scotland being a good example in the north). By looking at the map above, for example, we can have an educated guess about the presence of spatial autocorrelation; **but actually determining whether what we are seeing could have come from pure chance or not is usually easier said than done**.\n",
    "\n",
    "That is exactly the purpose of indicators of global spatial autocorrelation: to leverage the power of statistics to help us first summarize the spatial distribution of values present in a map, and second obtain a formal quantification of the departure from randomness. These are statistics to characterize a map in terms of its degree of clustering and summarize it, either in a visual or numerical way. However, before we can delve into the statistics, we need to understand a core building block: the **spatial lag**. With that concept under the belt, we are in a position to build a good understanding of global spatial autocorrelation. We will gently enter it with the binary case, when observations can only take two (potentially categorical) values, before we cover the two workhorses of the continuous case: the Moran Plot and Moran's I."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d808a",
   "metadata": {},
   "source": [
    "### Spatial Lag"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87ea6a24",
   "metadata": {},
   "source": [
    "The spatial lag operator is one of the most common and direct applications of spatial weights matrices ($\\textbf{W}$'s) in spatial analysis. The mathematical definition is the product of $\\textbf{W}$ and the vector of a given variable. Conceptually, the spatial lag captures the behavior of a variable in the immediate surroundings of each location; in that respect, it is **akin to a local smoother of a variable**. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "070a2295",
   "metadata": {},
   "source": [
    "We can formally express it in matrix notation as:\n",
    "\n",
    "$$\n",
    "Y_{sl} = \\textbf{W} Y\n",
    "$$\n",
    "\n",
    "or, in individual notation as:\n",
    "\n",
    "$$\n",
    "y_{sl-i} = \\sum_j w_{ij} y_j\n",
    "$$\n",
    "\n",
    "where $w_{ij}$ is the cell in $\\textbf{W}$ on the $i$-th row and $j$-th column, thus capturing the spatial relationship between observations $i$ and $j$. $y_{sl-i}$ thus captures the product of the values and weights of each observation other than $i$ in the dataset. Because non-neighbors receive a weight of zero, $y_{sl-i}$ really captures the product of values and weights for $i$'s neighbors. If $\\textbf{W}$ is binary, this will amount to the sum of the values of $i$'s neighbors (useful in some contexts, such as studies of market potential); if $W$ is row standardized, a common transformation, then $w_{ij}$ is bounded between zero and one; the spatial lag thus then becomes a \"**local average**,\" the average value of $Y$ in the neighborhood of each observation $i$. This latter meaning is the one that will enable our analysis of spatial autocorrelation below.\n",
    "\n",
    "As we will discover throughout this book, the spatial lag is a key element of many spatial analysis techniques and, as such, it is fully supported in Pysal. To compute the spatial lag of a given variable, `Pct_Leave` for example, we can do it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8118aa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "db[\"Pct_Leave_lag\"] = weights.spatial_lag.lag_spatial(\n",
    "    w, db[\"Pct_Leave\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614e958e",
   "metadata": {},
   "source": [
    "Let us peek into two local authority districts to get a better intuition of what is behind the spatial lag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240bcdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.loc[[\"E08000012\", \"S12000019\"], [\"Pct_Leave\", \"Pct_Leave_lag\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b4f0bc",
   "metadata": {},
   "source": [
    "The first row (`E08000012`) represents Liverpool, which was a notorious \"Remainer\" island among the mostly-Leave North of England. Outside of London and Scotland, it was one of the few locations with less than majority to Leave. The second row (`S12000019`) represents Midlothian, in Scotland, where no local authority voted to leave. Although both Liverpool and Midlothian display a similar percentage of population who voted to leave (42% and 38%, respectively), the difference in their spatial lags captures the wider geographical context, which are quite different.\n",
    "\n",
    "To end this section visually, the smoothing nature of the lag can be appreciated in the following map comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1facdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax1, ax2 = axs\n",
    "\n",
    "db.plot(\n",
    "    column=\"Pct_Leave\",\n",
    "    cmap=\"viridis\",\n",
    "    scheme=\"quantiles\",\n",
    "    k=5,\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=0.0,\n",
    "    alpha=0.75,\n",
    "    legend=True,\n",
    "    ax=ax1,\n",
    ")\n",
    "ax1.set_axis_off()\n",
    "ax1.set_title(\"% Leave\")\n",
    "\n",
    "db.plot(\n",
    "    column=\"Pct_Leave_lag\",\n",
    "    cmap=\"viridis\",\n",
    "    scheme=\"quantiles\",\n",
    "    k=5,\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=0.0,\n",
    "    alpha=0.75,\n",
    "    legend=True,\n",
    "    ax=ax2,\n",
    ")\n",
    "ax2.set_axis_off()\n",
    "ax2.set_title(\"% Leave - Spatial Lag\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fde86b0e",
   "metadata": {},
   "source": [
    "Stark differences on the left between immediate neighbors (as in the case of Liverpool, in the NW of England) are diminished on the map in the right. **Thus, as discussed above, the spatial lag can also smooth out the differences between nearby observations**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca282a5",
   "metadata": {},
   "source": [
    "### Binary case: join counts\n",
    "\n",
    "The spatial lag plays an important role in quantifying spatial autocorrelation. Using it, we can begin to relate the behavior of a variable at a given location to its pattern in the immediate neighborhood. Measures of global spatial autocorrelation will then use each observation to construct overall measures about the general trend in a given dataset. \n",
    "\n",
    "Our first dip into these measures considers a simplified case: binary values. This occurs when the variable we are interested in only takes two values. In this context, we are interested in whether a given observation is surrounded by others within the same category. For example, returning to our dataset, we want to assess the extent to which local authorities who voted to Leave tend to be surrounded by others who also voted to leave. To proceed, let us first calculate a binary variable (`Leave`) that indicates 1 if the local authority voted to leave, and zero otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ce2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "db[\"Leave\"] = (db[\"Pct_Leave\"] > 50).astype(int)\n",
    "db[[\"Pct_Leave\", \"Leave\"]].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d07f52",
   "metadata": {},
   "source": [
    "Which we can visualize readily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c7aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "db.plot(\n",
    "    ax=ax,\n",
    "    column=\"Leave\",\n",
    "    categorical=True,\n",
    "    legend=True,\n",
    "    edgecolor=\"0.5\",\n",
    "    linewidth=0.25,\n",
    "    cmap=\"Set3\",\n",
    "    figsize=(9, 9),\n",
    ")\n",
    "ax.set_axis_off()\n",
    "ax.set_title(\"Leave Majority\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e705ba6",
   "metadata": {},
   "source": [
    "Visually, it appears that the map represents a clear case of positive spatial autocorrelation: overall, there are few visible cases where a given observation is surrounded by others in the opposite category. To formally explore this initial assessment, we can use what is called a \"**join count**\" statistic (JC; {cite}`Cliff1981spatial`). \n",
    "\n",
    "Imagine a checkerboard with green (G, value 0) and yellow (Y, value 1) squares. The idea of the statistic is to count occurrences of green-green (GG), yellow-yellow (YY), or green-yellow/yellow-green (GY) joins (or neighboring pairs) on the map. In this context, both GG and YY reflect positive spatial autocorrelation, while GY captures its negative counterpart. The intuition of the statistic is to provide a baseline of how many GG, YY, and GY one would expect under the case of complete spatial randomness, and to compare this with the observed counts in the dataset. **A situation where we observe more GG/YY than expected and less GY than expected would suggest positive spatial autocorrelation; while the opposite, more GY than GG/YY, would point towards negative spatial autocorrelation.**\n",
    "\n",
    "Since the spatial weights are only used here to delimit who is a neighbor or not, the join count statistic requires binary weights. Let us thus transform `w` back to a non-standardized state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b61bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab8ada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.transform = \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c91a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71adf58",
   "metadata": {},
   "source": [
    "We can compute the statistic as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02799f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1234)\n",
    "jc = esda.join_counts.Join_Counts(db[\"Leave\"], w)\n",
    "jc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6587544d",
   "metadata": {},
   "source": [
    "As it is common throughout Pysal, we are creating an object (`jc`) that holds a lot of information beyond the value of the statistic calculated. For example, we can check how many occurrences of GG we have (note the attribute is `bb`, which originates from the original reference where the two considered classes were black and white):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8374ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "jc.bb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7034a61",
   "metadata": {},
   "source": [
    "how many YY occurrences our map has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3da8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "jc.ww"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ec2581",
   "metadata": {},
   "source": [
    "and how many GY/YG we find:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1e3062",
   "metadata": {},
   "outputs": [],
   "source": [
    "jc.bw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9553fe",
   "metadata": {},
   "source": [
    "The sum of those three gives us the total number of comparisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34d8588",
   "metadata": {},
   "outputs": [],
   "source": [
    "jc.bb + jc.ww + jc.bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b13442",
   "metadata": {},
   "outputs": [],
   "source": [
    "jc.J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edaf623",
   "metadata": {},
   "source": [
    "The statistic is based on comparing the actual number of joins of each class (`bb`, `ww`, `bc`) with what one would expect in a case of spatial randomness. Those expectations can be accessed as well, for the GG/YY case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bce7ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jc.mean_bb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9197e678",
   "metadata": {},
   "source": [
    "and for GY joins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6d867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jc.mean_bw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8073d1f1",
   "metadata": {},
   "source": [
    "Statistical inference to obtain a sense of whether these values are likely to come from random chance or not can be accessed using random spatial permutations of the observed values to create synthetic maps under the null hypothesis of complete spatial randomness. `esda` generates 999 such synthetic patterns and then uses the distribution of join counts from these patterns to generate pseudo-pvalues for our observed join count statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77839ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "jc.p_sim_bb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da6b1cf5",
   "metadata": {},
   "source": [
    "What's a **p-value** again? The p-value is the probability that we observe the statistic that we saw (727.412 in the base of bb and 649.32 in the case of bw) in a \"null hypothesis\" scenario where, in this case, all the bb, ww, and bws are randomly distributed. \n",
    "\n",
    "\n",
    "</figure>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/0/00/P-value_Graph.png\" alt=\"drawing\" width=\"600\" style=\"display: block; margin: 0 auto\"/>\n",
    "</figure>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694a02c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "jc.p_sim_bw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336f0c23",
   "metadata": {},
   "source": [
    "These results point to a clear presence of positive spatial autocorrelation, as there are a lot more joins of pairs in the same category than one would expect (`p_sim_bb`) and significantly less of opposite joins (`p_sim_bw`). We will discuss the generation of the pseudo p-values in more detail in the next section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9457d26c",
   "metadata": {},
   "source": [
    "### Continuous case: Moran Plot and Moran's I\n",
    "\n",
    "Once we have built some intuition around how spatial autocorrelation can be\n",
    "formally assessed in the binary case, let us move to situations where the\n",
    "variable of interest does not only take two values, but is instead continuous.\n",
    "Probably the most commonly used statistic in this context is Moran's I {cite}`Moran1948`, which can be written as:\n",
    "\n",
    "$$\n",
    "I = \\dfrac{n}{\\sum_i\\sum_j w_{ij}} \\dfrac{\\sum_i\\sum_j w_{ij} \\, z_i \\, z_j}{\\sum_i z_i^2}\n",
    "$$\n",
    "\n",
    "where $n$ is the  number of observations, $z_{i}$ is the standardized value of the variable of interest at location $i$, and $w_{ij}$ is the cell corresponding to the $i$-th row and $j$-th column of a $W$ spatial weights matrix.\n",
    "\n",
    "In order to understand the intuition behind its math, it is useful to begin with a graphical interpretation: the Moran Plot. The Moran Plot is a way of visualizing a spatial dataset to explore the nature and strength of spatial autocorrelation. It is essentially a traditional scatter plot in which the variable of interest is displayed against its *spatial lag*. In order to be able to interpret values as above or below the mean, the variable of interest is usually standardized by subtracting its mean and dividing by the standard deviation of the variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81209621",
   "metadata": {},
   "outputs": [],
   "source": [
    "db[\"Pct_Leave_std\"] = (db[\"Pct_Leave\"] - db[\"Pct_Leave\"].mean())/db[\"Pct_Leave\"].std()\n",
    "db[\"Pct_Leave_lag_std\"] = (\n",
    "    db[\"Pct_Leave_lag\"] - db[\"Pct_Leave_lag\"].mean()\n",
    ")/db[\"Pct_Leave_lag\"].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143cf710",
   "metadata": {},
   "source": [
    "Technically speaking, creating a Moran Plot is very similar to creating any other scatter plot in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc7b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(6, 6))\n",
    "sns.regplot(\n",
    "    x=\"Pct_Leave_std\",\n",
    "    y=\"Pct_Leave_lag_std\",\n",
    "    ci=None,\n",
    "    data=db,\n",
    "    line_kws={\"color\": \"r\"},\n",
    ")\n",
    "ax.axvline(0, c=\"k\", alpha=0.5)\n",
    "ax.axhline(0, c=\"k\", alpha=0.5)\n",
    "ax.set_title(\"Moran Plot - % Leave\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d421ae34",
   "metadata": {},
   "source": [
    "The figure above displays the relationship between the standardized \"Leave\" voting percentage in a local authority and its spatial lag which, because the $W$ used is row-standardized, can be interpreted as the **average standardized density of the percent Leave vote in the neighborhood of each observation**. In order to guide the interpretation of the plot, a linear fit is also included. This line represents the best linear fit to the scatter plot or, in other words, what is the best way to represent the relationship between the two variables as a straight line."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed594e39",
   "metadata": {},
   "source": [
    "The plot displays a positive relationship between both variables. This is indicates the presence of positive spatial autocorrelation: **similar values tend to be located close to each other. This means that the overall trend is for high values to be close to other high values, and for low values to be surrounded by other low values**. This, however, does not mean that this is the only case in the dataset: there can of course be particular situations where high values are surrounded by low ones, and *vice versa*. But it means that, if we had to summarize the main pattern of the data in terms of how clustered similar values are, the best way would be to say they are positively correlated and, hence, clustered over space. In the context of the example, this can be interpreted along the lines of: local authorities where people voted in high proportion to leave the EU tend to be located nearby other regions that also registered high proportions of Leave vote. In other words, **we can say the percentage of Leave votes is spatially autocorrelated in a positive way**.\n",
    "\n",
    "The Moran Plot is an excellent tool to explore the data and get a good sense of how much values are clustered over space. However, because it is a graphical device, it is sometimes hard to condense its insights into a more concise way. For these cases, a good approach is to come up with a statistical measure that summarizes the figure. This is exactly what Moran's I, as formally expressed above, is meant to do.\n",
    "\n",
    "Very much in the same way the mean summarizes a crucial element of the distribution of values in a non-spatial setting, so does Moran's I for a spatial dataset. Continuing the comparison, we can think of the mean as a single numerical value summarizing a histogram or a kernel density plot. Similarly, Moran's I captures much of the essence of the Moran Plot. In fact, there is a close connection between the two: **the value of Moran's I corresponds with the slope of the linear fit overlayed on top of the Moran Plot**.\n",
    "\n",
    "In order to calculate Moran's I in our dataset, we can call a specific function in `esda` directly (before that, let us row standardized the `w` object again):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4fe033",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.transform = \"R\"\n",
    "moran = esda.moran.Moran(db[\"Pct_Leave\"], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b20250",
   "metadata": {},
   "source": [
    "The method `Moran` creates an object that contains much more information than the actual statistic. If we want to retrieve the value of the statistic, we can do it this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21692588",
   "metadata": {},
   "outputs": [],
   "source": [
    "moran.I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea74e035",
   "metadata": {},
   "source": [
    "The other bit of information we will extract from Moran's I relates to statistical inference: how likely is the pattern we observe in the map and Moran's I captures in its value to be generated by an entirely random process? If we considered the same variable but shuffled its locations randomly, would we obtain a map with similar characteristics? To obtain insight into these questions, `esda` performs a simulation and returns a measure of certainty about how likely it is to obtain a pattern like the one we observe under a spatially random process.  This is summarized in the `p_sim` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4072c90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "moran.p_sim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e76a67e",
   "metadata": {},
   "source": [
    "The value is calculated as an empirical p-value that represents the proportion of realizations in the simulation under spatial randomness that are more extreme than the observed value. A small enough p-value associated with the Moran's I of a map allows to reject the hypothesis that the map is random. In other words, we can conclude that the map displays more spatial pattern than we would expect if the values had been randomly allocated to a locations.\n",
    "\n",
    "That is a very low value, particularly considering it is actually the minimum value we could have obtained given the simulation behind it used 999 permutations (default in `esda`) and, by standard terms, it would be deemed statistically significant. We can elaborate a bit further on the intuition behind the value of `p_sim`. **If we generated a large number of maps with the same values but randomly allocated over space, and calculated the Moran's I statistic for each of those maps, only 0.01% of them would display a larger (absolute) value than the one we obtain from the observed data, and the other 99.99% of the random maps would receive a smaller (absolute) value of Moran's I**. If we remember again that the value of Moran's I can also be interpreted as the slope of the Moran Plot, what we have is that, in this case, the particular spatial arrangement of values over space we observe for the percentage of Leave votes is more concentrated than if we were to randomly shuffle the vote proportions among the map, hence the statistical significance. As a first step, the global autocorrelation analysis can teach us that observations do seem to be positively autocorrelated over space. Indeed, the overall spatial pattern in the EU Referendum vote was highly marked: nearby areas tended to vote alike.\n",
    "\n",
    "Thanks to the `splot` visualization module in Pysal, we can obtain a quick representation of the statistic that combines the Moran Plot (right) with a graphic of the empirical test that we carry out to obtain `p_sim` (left):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227bd271",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_moran(moran);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4149a492",
   "metadata": {},
   "source": [
    "On the left panel we can see in gray the empirical distribution generated from simulating 999 random maps with the values of the `Pct_Leave` variable and then calculating Moran's I for each of those maps. The blue rug signals the mean. In contrary, the red rug shows Moran's I calculated for the variable using the geography observed in the dataset. It is clear the value under the observed pattern is significantly higher than under randomness. This insight is confirmed on the right panel, which shows an equivalent plot to the Moran Plot we created above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14d16f24",
   "metadata": {},
   "source": [
    "### Other global indices (skip during class)\n",
    "\n",
    "Moran's I is probably the most widely used statistic for global spatial autocorrelation, however it is not the only one. In this final part of the chapter, we introduce two additional measures that are common in applied work. Although they all consider spatial autocorrelation, they differ in how the concept is tackled in the specification of each test.\n",
    "\n",
    "#### Geary's C\n",
    "\n",
    "The contiguity ratio $c$, proposed by {cite}`Geary1954contiguity`, is given by:\n",
    "\n",
    "$$\n",
    "C = \\dfrac{(n-1)}\n",
    "          {2 \\sum_i \\sum_j w_{ij}} \n",
    "    \\dfrac{\\sum_i \\sum_j w_{ij} (y_i - y_{j})^2}\n",
    "          {\\sum_i (y_i - \\bar{y})^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4375a417",
   "metadata": {},
   "source": [
    "where $n$ is the number of observations, $w_{ij}$ is the cell in a binary matrix $W$ expressing whether $i$ and $j$ are neighbors ($w_{ij}=1$) or not ($w_{ij}=1$), $y_i$ is the $i$-th observation of the variable of interest, and $\\bar{y}$ is its sample mean. When compared to Moran's I, it is apparent both measures compare the relationship of $Y$ within each observation's local neighborhood to that over the entire sample. However, there are also subtle differences. While Moran's I takes cross-products on the standardized values, Geary's C uses differences on the values without any standardization. \n",
    "\n",
    "Computationally, Geary's C is more demanding, but it can be easily computed using `esda`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345926e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "geary = esda.geary.Geary(db[\"Pct_Leave\"], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18434fde",
   "metadata": {},
   "source": [
    "Which has a similar way of accessing its estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e959a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "geary.C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8514255",
   "metadata": {},
   "source": [
    "Inference is performed in a similar way as with Moran's I. We can perform a simulation that allows us to draw an empirical distribution of the statistic under the null of spatial randomness, and then compare it with the statistic obtained when using the observed geographical distribution of the data. To access the pseudo p-value, calculated as in the Moran case, we can call `p_sim`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "geary.p_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a5b28c",
   "metadata": {},
   "source": [
    "In this case, Geary's C points in the same direction as Moran's I: there is clear indication that the statistic we calculate on the observed dataset is different from what would be expected in a situation of pure spatial randomness. Hence, from this analysis, we can also conclude spatial autocorrelation is present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced43b36",
   "metadata": {},
   "source": [
    "#### Getis and Ord's G\n",
    "\n",
    "Originally proposed by {cite}`Getis1992analysis`, the $G$ is the global version of a family of statistics of spatial autocorrelation based on distance. The $G$ class of statistics is conceived for points, hence the use of a distance $W$, but it can also be applied to polygon data if a binary spatial weights matrix can be constructed. Additionally, it is designed for the study of positive variables with a natural origin. The $G$ can be expressed as follows:\n",
    "\n",
    "$$\n",
    "G(d) = \\dfrac{ \\sum_i \\sum_j w_{ij}(d) \\, y_i \\, y_j }\n",
    "             { \\sum_i \\sum_j y_i \\, y_j }\n",
    "$$\n",
    "\n",
    "where $w_{ij}(d)$ is the binary weight assigned on the relationship between observations $i$ and $j$ following a distance band criterion. $G$ was originally proposed as a measure of concentration rather than of spatial autocorrelation. As such, it is well suited to test to what extent similar values (either high or low) tend to co-locate. In other words, the $G$ is a statistic of *positive* spatial autocorrelation. This is usually the interest in most Geographic Data Science applications. However, it is important to note that, because $G$ can be understood as a measure of the intensity with which $Y$ is concentrated, the statistic is not able to pick up cases of *negative* spatial autocorrelation.\n",
    "\n",
    "To illustrate its computation, let us calculate a binary distance band $W$. To make sure every observation has at least one neighbor, we will use the `min_threshold_distance` method and project the dataset into the Ordnance Survey CRS (`EPSG` code 27700), expressed in meters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bb95d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_osgb = db.to_crs(epsg=27700)\n",
    "pts = db_osgb.centroid\n",
    "xys = pandas.DataFrame({\"X\": pts.x, \"Y\": pts.y})\n",
    "min_thr = weights.util.min_threshold_distance(xys)\n",
    "min_thr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a73145",
   "metadata": {},
   "source": [
    "For every local authority to have a neighbor, the distance band needs to at least be about 181 Km. This information can then be passed to the `DistanceBand` constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dedc237",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_db = weights.DistanceBand.from_dataframe(db_osgb, min_thr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d6cc30",
   "metadata": {},
   "source": [
    "At this point, we are ready to calculate the global $G$ statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cb20e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gao = esda.getisord.G(db[\"Pct_Leave\"], w_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d24929",
   "metadata": {},
   "source": [
    "Access to the statistic (`gao.G`) and additional attributes can be gained in the same way as with the previous statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aad773",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Getis & Ord G: %.3f | Pseudo P-value: %.3f\" % (gao.G, gao.p_sim)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5811b2fc",
   "metadata": {},
   "source": [
    "Similarly, inference can also be carried out by relying on computational simulations that replicate several instances of spatial randomness using the values in the variable of interest, but shuffling their locations. In this case, the pseudo P-value computed suggests a clear departure from the hypothesis of no concentration.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "996f7164",
   "metadata": {},
   "source": [
    "\n",
    "## Q.1 \n",
    "Sometimes referendums require more than 50% to make the change they ask about. Let us imagine the EU referendum required 60% to succeed on leaving the EU.\n",
    "1. Use `Pct_Leave` to create a binary variable that takes a value of 1 if the percentage was larger than 60, 0 otherwise. (2 pts)\n",
    "2. Create a choropleth with the newly created variable. Are there any differences in the geographical pattern of the vote to leave the EU? (You can just describe this without using any statistics) (2 pts)\n",
    "3. Re-compute the Join Counts statistic for this new variable. What can we conclude? Are there any notable changes in the extent to which \"Leave\" votes were distributed spatially? (2 pts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83376d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## insert your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26aac7b7",
   "metadata": {},
   "source": [
    "## Q.2\n",
    "\n",
    "Return to the original ref table and pull out the `Pct_Rejected` variable. Using $k$-nearest neighbor weights, can you find the $k$ where Moran's $I$ is largest? Use a range from 1 to 20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e7e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "## insert your code here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c0768d7",
   "metadata": {},
   "source": [
    "Make a plot of the Moran's $I$ for each $k$ you evaluate to show the relationship between the two.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fe0f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Insert you code here \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "463706a6",
   "metadata": {},
   "source": [
    "Explain the intuition for why you saw this relationship. (2 pts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7e56683",
   "metadata": {},
   "source": [
    "# Local Spatial Autocorrelation\n",
    "\n",
    "Local measures of spatial autocorrelation focus on the relationships between _each_ observation and its surroundings, rather than providing a single summary of these relationships across the map. In this sense, **they are not summary statistics but scores that allow us to learn more about the spatial structure in our data**. The general intuition behind the metrics however is similar to that of global ones. Some of them are even mathematically connected, where the global version can be decomposed into a collection of local ones. One such example are Local Indicators of Spatial Association (LISAs) {cite}`Anselin1995local`, which we use to build the understanding of local spatial autocorrelation, and on which we spend a good part of the chapter. \n",
    "\n",
    "Once such concepts are firmed, we introduce a couple alternative statistics that present complementary information or allow us to obtain similar insights for categorical data. Although very often these statistics are used with data expressed in geo-tables, there is nothing fundamentally connecting the two. In fact, the application of these methods to large surfaces is a promising area of work. For that reason, we close the chapter with an illustration of how one can run these statistics on data stored as surfaces."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b71c9aac",
   "metadata": {},
   "source": [
    "## An empirical illustration: the EU Referendum\n",
    "\n",
    "We continue with the same dataset about Brexit voting that we examined in the previous chapter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "075f1091",
   "metadata": {},
   "source": [
    "Although there are several variables that could be considered, we will focus on `Pct_Leave` again, which measures the proportion of votes in the UK Local Authority that wanted to Leave the European Union. With these elements, we can generate a choropleth to get a quick sense of the spatial distribution of the data we will be analyzing. Note how we use some visual tweaks (e.g., transparency through the `alpha` attribute) to make the final plot easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c069b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure and a single axis\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "# Build choropleth\n",
    "db.plot(\n",
    "    column=\"Pct_Leave\",\n",
    "    cmap=\"viridis\",\n",
    "    scheme=\"quantiles\",\n",
    "    k=5,\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=0.0,\n",
    "    alpha=0.75,\n",
    "    legend=True,\n",
    "    legend_kwds=dict(loc=2),\n",
    "    ax=ax,\n",
    ")\n",
    "# Add basemap\n",
    "# ctx.add_basemap(\n",
    "#     ax,\n",
    "#     crs=db.crs,\n",
    "#     source=ctx.providers.CartoDB.VoyagerNoLabels,\n",
    "# )\n",
    "# Remove axes\n",
    "ax.set_axis_off();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a07ef29d",
   "metadata": {},
   "source": [
    "Here, we will use eight nearest neighbors for the sake of the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2691321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate W from the GeoDataFrame\n",
    "w = weights.distance.KNN.from_dataframe(db, k=8)\n",
    "# Row-standardization\n",
    "w.transform = \"R\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6545559",
   "metadata": {},
   "source": [
    "## Motivating Local Spatial Autocorrelation\n",
    "\n",
    "To better understand the underpinnings of local spatial autocorrelation, we return to the Moran Plot as a graphical tool. In this context, it is more intuitive to represent the data in a standardised form, as it will allow us to more easily discern a typology of spatial structure. Let us first calculate the spatial lag of our variable of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f381be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "db[\"w_Pct_Leave\"] = weights.spatial_lag.lag_spatial(\n",
    "    w, db[\"Pct_Leave\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb78611",
   "metadata": {},
   "source": [
    "And their respective standardized versions, where we subtract the average and divide by the standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f207613",
   "metadata": {},
   "outputs": [],
   "source": [
    "db[\"Pct_Leave_std\"] = (db[\"Pct_Leave\"] - db[\"Pct_Leave\"].mean())/db[\"Pct_Leave\"].std()\n",
    "db[\"w_Pct_Leave_std\"] = (db[\"w_Pct_Leave\"] - db[\"w_Pct_Leave\"].mean())/db[\"w_Pct_Leave\"].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8390c94",
   "metadata": {},
   "source": [
    "Technically speaking, creating a Moran Plot is very similar to creating any other scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f54dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(6, 6))\n",
    "# Plot values\n",
    "sns.regplot(\n",
    "    x=\"Pct_Leave_std\", y=\"w_Pct_Leave_std\", data=db, ci=None\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c924a532",
   "metadata": {},
   "source": [
    "Using standardized values, we can immediately divide each variable (the percentage that voted to leave, and its spatial lag) in two groups: those with above-average leave voting, which have positive standardized values; and those with below-average leave voting, which feature negative standardized values. Applying this thinking to both the percentage to leave and its spatial lag, divides a Moran Plot in four quadrants. Each of them captures a situation based on whether a given area displays a value above the mean (high) or below (low) in either the original variable (`Pct_Leave`) or its spatial lag (`w_Pct_Leave_std`). Using this terminology, we name the four quadrants as follows: high-high (HH) for the top-right, low-high (LH) for the top-left, low-low (LL) for the bottom-left, and high-low (HL) for the bottom right. Graphically, we can capture this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c397413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(6, 6))\n",
    "# Plot values\n",
    "sns.regplot(\n",
    "    x=\"Pct_Leave_std\", y=\"w_Pct_Leave_std\", data=db, ci=None\n",
    ")\n",
    "# Add vertical and horizontal lines\n",
    "plt.axvline(0, c=\"k\", alpha=0.5)\n",
    "plt.axhline(0, c=\"k\", alpha=0.5)\n",
    "# Add text labels for each quadrant\n",
    "plt.text(2.0, .5, \"HH\", fontsize=25, c=\"r\")\n",
    "plt.text(1.2, -1.1, \"HL\", fontsize=25, c=\"r\")\n",
    "plt.text(-2.0, .8, \"LH\", fontsize=25, c=\"r\")\n",
    "plt.text(-2.5, -1.1, \"LL\", fontsize=25, c=\"r\")\n",
    "# Display\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9156692",
   "metadata": {},
   "source": [
    "## Local Moran's $I_i$\n",
    "\n",
    "The core idea of a local Moran's $I_i$ is to identify cases in which the value of an observation and the average of its surroundings is either more similar (HH or LL in the scatterplot above) or dissimilar (HL, LH) than we would expect from pure chance. The mechanism to do this is similar to the one in the global Moran's I, but applied in this case to each observation. This results in as many statistics as original observations. The formal representation of the statistic can be written as:\n",
    "\n",
    "$$\n",
    "I_i = \\dfrac{z_i}{m_2} \\displaystyle\\sum_j w_{ij} z_j \\; ; \\; m_2 = \\dfrac{\\sum_i z_i^2}{n}\n",
    "$$\n",
    "\n",
    "where $m_2$ is the second moment (variance) of the distribution of values in the data, $z_i = y_i - \\bar{y}$, $w_{i,j}$ is the spatial weight for the pair of observations $i$ and $j$, and $n$ is the number of observations.\n",
    "\n",
    "LISAs are widely used in many fields to identify geographical clusters of values or find geographical outliers. They are a useful tool that can quickly return areas in which values are concentrated and provide suggestive evidence about the processes that might be at work. For these reasons, they have a prime place in the geographic data science toolbox. Among many other applications, LISAs have been used to identify geographical clusters of poverty {cite}`Dawson2018`, map ethnic enclaves {cite}`Johnston2010EPA`, delineate areas of particularly high/low economic activity {cite}`Torres2014`, or identify clusters of contagious disease {cite}`zhang2020`. The Local Moran's $I_i$ statistic is only one of a wide variety of LISAs that can be used on many different types of spatial data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ae7bb",
   "metadata": {},
   "source": [
    "In Python, we can calculate LISAs in a very streamlined way thanks to `esda`. To compute local Moran statistics, we use the `Moran_Local` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e5facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lisa = esda.moran.Moran_Local(db[\"Pct_Leave\"], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcae2a4",
   "metadata": {},
   "source": [
    "We need to pass the variable of interestproportion of Leave votes in this contextand the spatial weights that describes the neighborhood relations between the different areas that make up the dataset. This creates a LISA object (`lisa`) that has a number of attributes of interest. The local indicators themselves are in the `Is` attribute and we can get a sense of their distribution using `seaborn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80714d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw KDE line\n",
    "ax = sns.kdeplot(lisa.Is)\n",
    "# Add one small bar (rug) for each observation\n",
    "# along horizontal axis\n",
    "sns.rugplot(lisa.Is, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b396d4e",
   "metadata": {},
   "source": [
    "The figure reveals a rather skewed distribution of local Moran's $I_i$ statistics. This outcome is due to the dominance of positive forms of spatial association, implying most of the local statistic values will be positive. Here it is important to keep in mind that the high positive values arise from value similarity in space, and this can be due to either high values being next to high values *or* low values next to low values. The local $I_i$ values alone cannot distinguish these two cases.\n",
    "\n",
    "The values in the left tail of the density represent locations displaying negative spatial association. There are also two forms, a high value surrounded by low values, or a low value surrounded by high valued neighboring observations. And, again, the  $I_i$ statistic cannot distinguish between the two cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f36c0",
   "metadata": {},
   "source": [
    "Because of their very nature, looking at the numerical result of LISAs is not always the most useful way to exploit all the information they can provide. Remember we are calculating a statistic for every single observation in the data so, if we have many of them, it will be difficult to extract any meaningful pattern. In this context, a choropleth can help. At first glance, this may seem to suggest that a choropleth of the $I_i$  values would be a useful way to visualize the spatial distribution. We can see such map in the top-left panel of the figure below and, while it tells us whether the local association is positive (HH/LL) or negative (HL/LH), it cannot tell, for example, whether the yellow areas in Scotland are similar to those in the eastern cluster of yellow areas. Are the two experiencing similar patterns of spatial association, or is one of them HH and the other LL? Also, we know that values around zero will not be statistically significant. Which local statistics are thus significant and which ones non-significant from a statistical point of view? In other words, which ones can be considered statistical clusters and which ones mere noise?\n",
    "\n",
    "To answer these questions, we need to bring in additional information that we have computed when calculating the LISA statistics. We do this in four acts. The first one we have already mentioned: a straighforward choropleth of the local statistic of each area. The other three include information on the quadrant each area is assigned into, whether the statistic is considered significant or not, and a combination of those two in a single so-called _cluster_ map. A handy tool in this context is the `splot` library, part of the Pysal family, which provides a lightweight visualisation layer for spatial statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5551691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from splot import esda as esdaplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f5136",
   "metadata": {},
   "source": [
    "With all pieces in place, let's first get busy building the figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a596a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure and axes\n",
    "f, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 12))\n",
    "# Make the axes accessible with single indexing\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Subplot 1 #\n",
    "# Choropleth of local statistics\n",
    "# Grab first axis in the figure\n",
    "ax = axs[0]\n",
    "# Assign new column with local statistics on-the-fly\n",
    "db.assign(\n",
    "    Is=lisa.Is\n",
    "    # Plot choropleth of local statistics\n",
    ").plot(\n",
    "    column=\"Is\",\n",
    "    cmap=\"plasma\",\n",
    "    scheme=\"quantiles\",\n",
    "    k=5,\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=0.1,\n",
    "    alpha=0.75,\n",
    "    legend=True,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# Subplot 2 #\n",
    "# Quadrant categories\n",
    "# Grab second axis of local statistics\n",
    "ax = axs[1]\n",
    "# Plot Quandrant colors (note to ensure all polygons are assigned a\n",
    "# quadrant, we \"trick\" the function by setting significance level to\n",
    "# 1 so all observations are treated as \"significant\" and thus assigned\n",
    "# a quadrant color\n",
    "esdaplot.lisa_cluster(lisa, db, p=1, ax=ax)\n",
    "\n",
    "# Subplot 3 #\n",
    "# Significance map\n",
    "# Grab third axis of local statistics\n",
    "ax = axs[2]\n",
    "#\n",
    "# Find out significant observations\n",
    "labels = pd.Series(\n",
    "    1 * (lisa.p_sim < 0.05),  # Assign 1 if significant, 0 otherwise\n",
    "    index=db.index  # Use the index in the original data\n",
    "    # Recode 1 to \"Significant and 0 to \"Non-significant\"\n",
    ").map({1: \"Significant\", 0: \"Non-Significant\"})\n",
    "# Assign labels to `db` on the fly\n",
    "db.assign(\n",
    "    cl=labels\n",
    "    # Plot choropleth of (non-)significant areas\n",
    ").plot(\n",
    "    column=\"cl\",\n",
    "    categorical=True,\n",
    "    k=2,\n",
    "    cmap=\"Paired\",\n",
    "    linewidth=0.1,\n",
    "    edgecolor=\"white\",\n",
    "    legend=True,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "\n",
    "# Subplot 4 #\n",
    "# Cluster map\n",
    "# Grab second axis of local statistics\n",
    "ax = axs[3]\n",
    "# Plot Quandrant colors In this case, we use a 5% significance\n",
    "# level to select polygons as part of statistically significant\n",
    "# clusters\n",
    "esdaplot.lisa_cluster(lisa, db, p=0.05, ax=ax)\n",
    "\n",
    "# Figure styling #\n",
    "# Set title to each subplot\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(\n",
    "        [\n",
    "            \"Local Statistics\",\n",
    "            \"Scatterplot Quadrant\",\n",
    "            \"Statistical Significance\",\n",
    "            \"Moran Cluster Map\",\n",
    "        ][i],\n",
    "        y=0,\n",
    "    )\n",
    "# Tight layout to minimise in-betwee white space\n",
    "f.tight_layout()\n",
    "\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49468b58",
   "metadata": {},
   "source": [
    "The purple and yellow locations in the top-right map display the largest magnitude (positive and negative values) for the local statistics $I_i$. Yet, remember this signifies positive spatial autocorrelation, which can be of high _or_ low values. This map thus cannot distinguish between areas with low support for the Brexit vote and those highly in favour.\n",
    "\n",
    "To distinguish between these two cases, the map in the upper-right shows the location of the LISA statistic in the quadrant of the Moran Scatter plot. This indicates whether the positive (or negative) local association exists within a specific *quadrant*, such as the High-High quadrant. This information is recorded in the `q` attribute of the `lisa` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce885fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lisa.q[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4549902e",
   "metadata": {},
   "source": [
    "The correspondence between the numbers in the `q` attribute and the actual quadrants is as follows: `1` represents observations in the HH quadrant, `2` those in the LH one, `3` in the LL region, and `4` in the HL quadrant. Comparing the two maps in the top row reveals that the positive local association in Scotland is due to low support for Brexit, while the positive local association in the south is among local authorities that strongly support Brexit. Overall, we can obtain counts of areas in each quadrant as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a71a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.value_counts(lisa.q)\n",
    "counts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0613197",
   "metadata": {},
   "source": [
    "Showing that the high-high (1), and low-low (3), values are predominant. Care must be taken, however, in the interpretation of these first two maps, as the underlying **statistical significance of the local values has not been considered**. We have simply mapped the raw LISA value alongside the quadrant in which the local statistic resides. To statistical significance, the bottom left map distinguishes those polygons whose pseudo p-value is above (_\"Non-Significant\"_) or below (_\"Significant\"_) the threshold value of 5% we use in this context. An examination of the map suggests that quite a few local authorities have local statistics that are small enough so as to be compatible with pure chance. \n",
    "\n",
    "**Therefore, in order to focus on the areas that are most promising, we need to include significance information alongside the quadrant and local statistic.** Together, this \"cluster map\" (as it is usually called) extracts significant observations -those that are highly unlikely to have come from pure chance- and plots them with a specific color depending on their quadrant category. All of the needed pieces are contained inside the `lisa` object we have created above and, if passed in tandem with the geo-table containing the geographies it relates to, `splot` will make a cluster map for us.\n",
    "\n",
    "Reading the clustermap reveals a few interesting aspects that would have been hard to grasp by looking at the other maps only and that are arguably more relevant for an analysis of the data. First, **fewer than half of polygons that have degrees of local spatial association strong enough to reject the idea of pure chance**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1978eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "(lisa.p_sim < 0.05).sum() * 100 / len(lisa.p_sim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbcd22c6",
   "metadata": {},
   "source": [
    "A little over 42% of the local authorities are considered, by this analysis, to be part of a spatial cluster. Second, we identify three clear areas of low support for leaving the EU: Scotland, London, and the area around Oxford (North-West of London). And third, although there appeared to be many areas with concentrated values indicating high support, it is only the region in the North-East and West of England whose spatial concentration shows enough strength to reasonably rule out pure chance.\n",
    "\n",
    "Before we move on from the LISA statistics, let's dive into a bit of the data engineering required to \"export\" significance levels and other information, as well as dig a bit further into what these numbers represent. The latter is useful if we need to work with them as part of a broader data pipeline. So far, cluster maps have been handled by `splot`, but there is quite a bit that happens under the hood. If we needed to recreate one of its maps, or to use this information in a different context, we would need to extract them out of our `lisa` object, and link them up to the original `db` table. Here is one way you can do this.\n",
    "\n",
    "First, we pull the information computed in `lisa` and insert it in the main data table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb1f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign pseudo P-values to `db`\n",
    "db[\"p-sim\"] = lisa.p_sim\n",
    "# `1` if significant (at 5% confidence level), `0` otherwise\n",
    "sig = 1 * (lisa.p_sim < 0.05)\n",
    "# Assign significance flag to `db`\n",
    "db[\"sig\"] = sig\n",
    "# Print top of the table to inspect\n",
    "db[[\"sig\", \"p-sim\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df5807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print bottom of the table to inspect\n",
    "db[[\"sig\", \"p-sim\"]].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45ededa",
   "metadata": {},
   "source": [
    "Thus, the first five values are statistically significant, while the last five observations are not.\n",
    "\n",
    "Let us stop for a second on these two steps. First, we consider the `sig` column. Akin to global Moran's I, `esda` automatically computes a pseudo p-value for each LISA. Because some instances of the LISA statistics may not be statistically significant, we want to identify those with a p-value small enough that rules out the possibility of obtaining a similar value in random maps. A few different ways of generating random maps are considered by `esda`, but we focus on a strategy that actually simulates hundreds of thousands of random maps to get a rough idea of the possible local statistic values at each local authority given the data we saw. In addition, we follow a similar reasoning as with global Moran's I and use 5% as the threshold for statistical significance. To identify these values, we create a variable, `sig`, that contains `True` if the p-value of the observation satisfies the condition, and `False` otherwise.\n",
    "\n",
    "Next, we construct our quadrant values using the `q` attribute which records the Moran Scatterplot quadrant for each local value. However, we now mask these values using the newly created binary significance measure `sig`, so only observations in a quadrant that are considered significant are labeled as part of that given quadrant. The remainder are labelled as non-significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bee7599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick as part of a quadrant only significant polygons,\n",
    "# assign `0` otherwise (Non-significant polygons)\n",
    "spots = lisa.q * sig\n",
    "# Mapping from value to name (as a dict)\n",
    "spots_labels = {\n",
    "    0: \"Non-Significant\",\n",
    "    1: \"HH\",\n",
    "    2: \"LH\",\n",
    "    3: \"LL\",\n",
    "    4: \"HL\",\n",
    "}\n",
    "# Create column in `db` with labels for each polygon\n",
    "db[\"labels\"] = pandas.Series(\n",
    "    # First initialise a Series using values and `db` index\n",
    "    spots,\n",
    "    index=db.index\n",
    "    # Then map each value to corresponding label based\n",
    "    # on the `spots_labels` mapping\n",
    ").map(spots_labels)\n",
    "# Print top for inspection\n",
    "db[\"labels\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982d777f",
   "metadata": {},
   "source": [
    "These cluster labels are meaningful if you know of the Moran Plot. To help making them a bit more intuitive, a terminology that is sometimes used goes as follows. Positive forms of local spatial autocorrelation are of two types. First, HH observations, which we can term \"hot spots\", represent areas where values at the site and its surroundings are larger than average. Second, LL observations, significant clusters of low values surrounded by low values, are sometimes referred to as \"cold spots\". Negative forms of local spatial autocorrelation also include two cases. When the focal observation displays low values but its surroundings have high values (LH), we call them \"doughnuts\". Conversely, areas with high values but neighboured by others with low values (HL) can be referred to as \"diamonds in the rough\". We note this terminology is purely mnemonic, but recognise in some cases it can help remembering the interpretation of local statistics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec20bb64",
   "metadata": {},
   "source": [
    "After building these new columns, analysis on the overall trends of LISA statistics is more straightforward than from the `lisa` object. For example, an overview of the distribution of labels is one line away:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d625c145",
   "metadata": {},
   "outputs": [],
   "source": [
    "db[\"labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575fec5c",
   "metadata": {},
   "source": [
    "This shows, for one, that most local statistics are *not* statistically significant. Among those that are, we see many more hotspots/coldspots than doughnuts/diamonds-in-the-rough. This is consistent with the skew we saw in the distribution of local statistics earlier. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "309b5849",
   "metadata": {},
   "source": [
    "# Q2\n",
    "Do the same Local Moran analysis done for `Pct_Leave`, but using `Pct_Turnout`. Is there a geography to how involved people were in different places? Where was turnout percentage (relatively) higher or lower? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be353f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db2 = (\n",
    "    gpd.GeoDataFrame(\n",
    "        lads.join(ref[[\"Pct_Turnout\"]]), crs=lads.crs\n",
    "    )\n",
    "    .to_crs(epsg=3857)[\n",
    "        [\"objectid\", \"lad16nm\", \"Pct_Turnout\", \"geometry\"]\n",
    "    ]\n",
    "    .dropna()\n",
    ")\n",
    "# Generate W from the GeoDataFrame\n",
    "w2 = weights.distance.KNN.from_dataframe(db2, k=8)\n",
    "# Row-standardization\n",
    "w2.transform = \"R\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde5283",
   "metadata": {},
   "outputs": [],
   "source": [
    "## insert code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4a2ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure and axes\n",
    "f, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 12))\n",
    "# Make the axes accessible with single indexing\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Subplot 1 #\n",
    "# Choropleth of local statistics\n",
    "# Grab first axis in the figure\n",
    "ax = axs[0]\n",
    "# Assign new column with local statistics on-the-fly\n",
    "db.assign(\n",
    "    Is=lisa2.Is\n",
    "    # Plot choropleth of local statistics\n",
    ").plot(\n",
    "    column=\"Is\",\n",
    "    cmap=\"plasma\",\n",
    "    scheme=\"quantiles\",\n",
    "    k=5,\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=0.1,\n",
    "    alpha=0.75,\n",
    "    legend=True,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# Subplot 2 #\n",
    "# Quadrant categories\n",
    "# Grab second axis of local statistics\n",
    "ax = axs[1]\n",
    "# Plot Quandrant colors (note to ensure all polygons are assigned a\n",
    "# quadrant, we \"trick\" the function by setting significance level to\n",
    "# 1 so all observations are treated as \"significant\" and thus assigned\n",
    "# a quadrant color\n",
    "esdaplot.lisa_cluster(lisa, db, p=1, ax=ax)\n",
    "\n",
    "# Subplot 3 #\n",
    "# Significance map\n",
    "# Grab third axis of local statistics\n",
    "ax = axs[2]\n",
    "#\n",
    "# Find out significant observations\n",
    "labels = pd.Series(\n",
    "    1 * (lisa.p_sim < 0.05),  # Assign 1 if significant, 0 otherwise\n",
    "    index=db.index  # Use the index in the original data\n",
    "    # Recode 1 to \"Significant and 0 to \"Non-significant\"\n",
    ").map({1: \"Significant\", 0: \"Non-Significant\"})\n",
    "# Assign labels to `db` on the fly\n",
    "db.assign(\n",
    "    cl=labels\n",
    "    # Plot choropleth of (non-)significant areas\n",
    ").plot(\n",
    "    column=\"cl\",\n",
    "    categorical=True,\n",
    "    k=2,\n",
    "    cmap=\"Paired\",\n",
    "    linewidth=0.1,\n",
    "    edgecolor=\"white\",\n",
    "    legend=True,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "\n",
    "# Subplot 4 #\n",
    "# Cluster map\n",
    "# Grab second axis of local statistics\n",
    "ax = axs[3]\n",
    "# Plot Quandrant colors In this case, we use a 5% significance\n",
    "# level to select polygons as part of statistically significant\n",
    "# clusters\n",
    "esdaplot.lisa_cluster(lisa, db, p=0.05, ax=ax)\n",
    "\n",
    "# Figure styling #\n",
    "# Set title to each subplot\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(\n",
    "        [\n",
    "            \"Local Statistics\",\n",
    "            \"Scatterplot Quadrant\",\n",
    "            \"Statistical Significance\",\n",
    "            \"Moran Cluster Map\",\n",
    "        ][i],\n",
    "        y=0,\n",
    "    )\n",
    "# Tight layout to minimise in-betwee white space\n",
    "f.tight_layout()\n",
    "\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21e14850",
   "metadata": {},
   "source": [
    "## Q.3 \n",
    "Local statistics use *permutation-based* inference for their significance testing. This means that, to test the statistical significance of a local relationship, values of the observed variable are *shuffled* around the map. These large numbers of *random* maps are then used to compare against the observed map. Local inference requires some restrictions on how each shuffle occurs, since each observation must be \"fixed\" and compared to randomly-shuffle neighboring observations. The distribution of local statistics for each \"shuffle\" is contained in the `.rlisas` attribute of a Local Moran object. \n",
    "    \n",
    "    \n",
    "For the first observation/column in `lisas`, make a `sns.distplot` of its shuffled local statistics. Add a vertical line to the histogram using `plt.axvline()`.  (5pts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d543862",
   "metadata": {},
   "outputs": [],
   "source": [
    "## insert code here\n",
    "## Assign a variable \"shuffles\" to lisa.rlisas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1734f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(shuffles[0,:])\n",
    "plt.axvline()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f099da4",
   "metadata": {},
   "source": [
    "Do the same for the last observation as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3558323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## insert code here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78c5f8af",
   "metadata": {},
   "source": [
    "Looking only at their permutation distributions, do you expect the first LISA statistic to be statistically-significant? Do you expect the last? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1f7c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## insert code here\n",
    "## Get the first value in lisa.Is (the LISA indicator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf934f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the first value in lisa.Is (the LISA indicator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1782bb89",
   "metadata": {},
   "source": [
    "INSERT YOUR TEXT EXPLANATION HERE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5b5e944a9b71ce46e873efe99abd044b4b2cd2a5153fea1f2fc73581012ba31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
